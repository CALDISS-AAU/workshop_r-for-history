{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d88720fd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## EXERCISE: REFRESHER\n",
    "\n",
    "1. Find two text snippets and assign each to its own object (fx text snippets from https://gutenberg.org/files/22381/22381.txt)\n",
    "\n",
    "2. Determine which text snippet is longest using the function `nchar()`\n",
    "\n",
    "3. Use a logical operator to get R to tell you, whether your text snippets have more than 400 characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5319a64",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## EXERCISE: REGULAR EXPRESSIONS\n",
    "\n",
    "1. Write a regular expression pattern that matches words beginning with \"d\" with a minimum length of 5 characters.\n",
    "    -  Use `str_extract_all()` to test whether it works\n",
    "\n",
    "\n",
    "2. Use `str_count` together with your regular expression to determine, how many matches there are in your text snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b219999a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## EXERCISE: STRINGS AS VECTORS\n",
    "\n",
    "1. Convert your two text snippets from earlier to a vector of senteces.\n",
    "\n",
    "    a. Put both texts into a vector using `c()`. Assign to an object.\n",
    "    \n",
    "    b. Split the texts into sentences using `str_split(texts, pattern = \", |\\\\. \")`. Assign to an object.\n",
    "    \n",
    "    c. Unlist the object to convert to a vector using `unlist()`. Assign to the same or a new object.\n",
    "    \n",
    "    \n",
    "2. Use `str_subset` along with regular expression to locate sentences containing a word starting with upper-case\n",
    "\n",
    "**Bonus**\n",
    "- Can you write the regular expression in a way that avoids including the words following a period?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4fc701",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## EXERCISE: SIMPLE TEXT MINING\n",
    "You will be repeating a lot of the same steps as you just saw. The goal is to figure out what names are used the most in the text \"Pagan and Christian Rome\" by Rodolfo Lanciani.\n",
    "\n",
    "Make sure the following packages are installed and loaded: `gutenbergr`, `tidytext`, `tidyverse`, `stringr`\n",
    "\n",
    "1. Load the text \"Pagan and Christian Rome\" from gutenberg into an object using `gutenberg_download(22153)`\n",
    "2. Use `unnest_tokens()` to convert the text to word tokens *without* converting to lower case (consult the help file with `?()` to see what option to change)\n",
    "3. Use `count` to create a data frame with the word tokens counted: `count(df, word, sort = TRUE)`\n",
    "4. Combine `filter()` and `str_detect()` to only keep words starting with uppercase letter (use the pattern `\"^[A-Z]\"`)\n",
    "5. Determine the most mentioned names (if you sorted the data, you can just print the top rows with `head()`)\n",
    "\n",
    "**Bonus (for a better result)**\n",
    "1. Try using `filter()` and `nchar()` to only keep words longer than 3 characters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
